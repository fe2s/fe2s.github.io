<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kafka, | Shady Minds]]></title>
  <link href="http://fe2s.github.io/blog/categories/kafka/atom.xml" rel="self"/>
  <link href="http://fe2s.github.io/"/>
  <updated>2014-08-21T18:50:33+03:00</updated>
  <id>http://fe2s.github.io/</id>
  <author>
    <name><![CDATA[Oleksiy Dyagilev]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[GigaSpaces With Kafka]]></title>
    <link href="http://fe2s.github.io/blog/2014/05/14/xap-kafka/"/>
    <updated>2014-05-14T16:46:56+03:00</updated>
    <id>http://fe2s.github.io/blog/2014/05/14/xap-kafka</id>
    <content type="html"><![CDATA[<p><a href="http://kafka.apache.org/">Apache Kafka</a> is a distributed publish-subscribe messaging system. It is designed to support persistent messaging with a O(1) disk structures that provides constant time performance even with many TB of stored messages. Apache Kafka provides High-throughput even with very modest hardware, Kafka can support hundreds of thousands of messages per second. Apache Kafka supports partitioning the messages over Kafka servers and distributing consumption over a cluster of consumer machines while maintaining per-partition ordering semantics. Many times Apache Kafka is used to perform parallel data load into Hadoop.</p>

<p>This pattern integrates <a href="http://www.gigaspaces.com/">GigaSpaces</a> with Apache Kafka. GigaSpacesâ€™ write-behind IMDG operations to Kafka making it available for the subscribers. Such could be Hadoop or other data warehousing systems using the data for reporting and processing. Sources are available on <a href="https://github.com/fe2s/xap-kafka">github</a></p>

<!-- more -->


<h2>XAP Kafka Integration Architecture</h2>

<p>The XAP Kafka integration is done via the <code>SpaceSynchronizationEndpoint</code> interface deployed as a Mirror service PU. It consumes a batch of IMDG operations, converts them to custom Kafka messages and sends these to the Kafka server using the Kafka Producer API.</p>

<p><img src="/images/xap-kafka/xap-kafka.jpg"  /></p>

<p>GigaSpace-Kafka protocol is simple and represents the data and its IMDG operation. The message consists of the IMDG operation type (Write, Update , remove, etc.) and the actual data object. The Data object itself could be represented either as a single object or as a Space Document with key/values pairs (<code>SpaceDocument</code>).
Since a Kafka message is sent over the wire, it should be serialized to bytes in some way.
The default encoder utilizes Java serialization mechanism which implies Space classes (domain model) to be <code>Serializable</code>.</p>

<p>By default Kafka messages are uniformly distributed across Kafka partitions. Please note, even though IMDG operations appear ordered in <code>SpaceSynchronizationEndpoint</code>, it doesn&rsquo;t imply correct ordering of data processing in Kafka consumers. See below diagram:</p>

<p><img src="/images/xap-kafka/xap-kafka-ordering.jpg"  /></p>

<h2>Getting started</h2>

<h3>Download the Kafka Example</h3>

<p>You can download the example code from <a href="https://github.com/fe2s/xap-kafka">github</a>.
The example located under <code>&lt;project_root&gt;/example</code>. It demonstrates how to configure Kafka persistence and implements a simple Kafka consumer pulling data from Kafka and store in HsqlDB.</p>

<h3>Running the Example</h3>

<p>In order to run an example, please follow the instruction below:</p>

<p>Step 1: Install Kafka<br/></p>

<p>Step 2: Start Zookeeper and Kafka server
<code>sh
bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-list-topic.sh --zookeeper localhost:2181
</code></p>

<p>Step 3: Build project
<code>sh
cd &lt;project_root&gt;
mvn clean install
</code></p>

<p>Step 4: Deploy example to GigaSpaces
<code>
cd example
mvn os:deploy
</code></p>

<p>Step 5: Check GigaSpaces log files, there should be messages from the Feeder and Consumer.</p>

<h2>Configuration</h2>

<h3>Library Dependency</h3>

<p>The following maven dependency needs to be included in your project in order to use Kafka persistence. This artifact is built from <code>&lt;project_rood&gt;/kafka-persistence</code> source directory.</p>

<pre><code class="xml">&lt;dependency&gt;
    &lt;groupId&gt;com.epam&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-persistence&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h2>Mirror service</h2>

<p>Here is an example of the Kafka Space Synchronization Endpoint configuration:</p>

<p>&#8220;`xml
<bean id="kafkaSpaceSynchronizationEndpoint" class="com.epam.openspaces.persistency.kafka.KafkaSpaceSynchronizationEndpointFactoryBean">
    <property name="producerProperties">
        <props>
            <prop key="metadata.broker.list"> localhost:9092</prop>
            <prop key="request.required.acks">1</prop>
        </props>
    </property>
</bean></p>

<!--
    The mirror space. Uses the Kafka external data source. Persists changes done on the Space that
    connects to this mirror space into the Kafka.
-->


<p>&lt;os-core:mirror id=&ldquo;mirror&rdquo; url=&ldquo;/./mirror-service&rdquo; space-sync-endpoint=&ldquo;kafkaSpaceSynchronizationEndpoint&rdquo; operation-grouping=&ldquo;group-by-replication-bulk&rdquo;>
    &lt;os-core:source-space name=&ldquo;space&rdquo; partitions=&ldquo;2&rdquo; backups=&ldquo;1&rdquo;/>
&lt;/os-core:mirror>
&#8220;`</p>

<p>Please consult Kafka documentation for the full list of available producer properties.
You can override the default properties if there is a need to customize GigaSpace-Kafka protocol. See Customization section below for details.</p>

<h3>Space class</h3>

<p>In order to associate a Kafka topic with the domain model class, the class needs to be annotated with the <code>@KafkaTopic</code> annotation and declared as <code>Serializable</code>. Here is an example</p>

<pre><code class="java">@KafkaTopic("user_activity")
@SpaceClass
public class UserActivity implements Serializable {
    ...
}
</code></pre>

<h3>Space Documents</h3>

<p>To configure a Kafka topic for a SpaceDocuments or Extended SpaceDocument, the property <code>KafkaPersistenceConstants.SPACE_DOCUMENT_KAFKA_TOPIC_PROPERTY_NAME</code> should be added to document. Here is an example</p>

<pre><code class="java">public class Product extends SpaceDocument {

public Product() {
    super("Product");
    super.setProperty(SPACE_DOCUMENT_KAFKA_TOPIC_PROPERTY_NAME, "product");
}
</code></pre>

<p>It&rsquo;s also possible to configure the name of the property which defines the Kafka topic for SpaceDocuments. Set <code>spaceDocumentKafkaTopicName</code> to the desired value as shown below.</p>

<pre><code class="xml">&lt;bean id="kafkaSpaceSynchronizationEndpoint" class="com.epam.openspaces.persistency.kafka.KafkaSpaceSynchrspaceDocumentKafkaTopicNameonizationEndpointFactoryBean"&gt;
    ...
    &lt;property name="spaceDocumentKafkaTopicName" value="topic_name" /&gt;
&lt;/bean&gt;
</code></pre>

<h2>Kafka consumers</h2>

<p>The Kafka persistence library provides a wrapper around the native Kafka Consumer API for the GigaSpace-Kafka protocol serialization. Please see <code>com.epam.openspaces.persistency.kafka.consumer.KafkaConsumer</code>, example of how to use it under <code>&lt;project_root&gt;/example module</code>.</p>

<h2>Customization</h2>

<ul>
<li>Kafka persistence was designed to be extensible and customizable.</li>
<li>If you need to create a custom protocol between GigaSpace and Kafka, provide an implementation of <code>AbstractKafkaMessage</code>, <code>AbstractKafkaMessageKey</code>, <code>AbstractKafkaMessageFactory</code>.</li>
<li>If you would like to customize how data grid operations are sent to Kafka or how the Kafka topic is chosen for a given entity, provide an implementation of &lsquo;AbstractKafkaSpaceSynchronizationEndpoint&rsquo;.</li>
<li>If you want to create a custom serializer, look at <code>KafkaMessageDecoder</code> and <code>KafkaMessageKeyDecoder</code>.</li>
<li>Kafka Producer client (which is used under the hood) can be configured with a number of settings, see Kafka documentation.</li>
</ul>

]]></content>
  </entry>
  
</feed>
