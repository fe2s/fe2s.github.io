<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: spark, | Shady Minds]]></title>
  <link href="http://dyagilev.org/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://dyagilev.org/"/>
  <updated>2018-09-28T18:08:09+03:00</updated>
  <id>http://dyagilev.org/</id>
  <author>
    <name><![CDATA[Oleksiy Dyagilev]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Real-time Spatial Analytics with InsightEdge Spark: Taxi Price Surge Use Case]]></title>
    <link href="http://dyagilev.org/blog/2016/09/29/real-time-spatial-analytics-with-insightedge-spark-taxi-price-surge-use-case/"/>
    <updated>2016-09-29T16:35:41+03:00</updated>
    <id>http://dyagilev.org/blog/2016/09/29/real-time-spatial-analytics-with-insightedge-spark-taxi-price-surge-use-case</id>
    <content type="html"><![CDATA[<p>A couple of weeks ago we <a href="http://insightedge.io/blog/insightedge-1-0-ga-real-time-insights-actionable-analytics/">launched InsightEdge</a>, introducing you to our high performance Spark distribution with enterprise-grade OLTP capabilities. In this blog post, we will create a demo application for a taxi price surge use case that runs real-time analytics on a streaming geospatial data.</p>

<!-- more -->

<p>We take a fundamental supply and demand economic model of price determination in a market. We will then compute price in real-time based on the current supply and demand.</p>

<p>To make our demo even more fun, we will create a taxi price surge use case. We will consider the transportation business domain, and taxi companies like Uber or Lyft in particular.</p>

<p>In taxi services, the order requests and available drivers represent the supply and demand data correspondingly. It is interesting that this data is bound to geographical location, which introduces additional complexity. Comparing to business areas like retail, where the product demand is linked to either offline store or a well known list of warehouses, the order requests are geographically distributed.</p>

<p>With services like Uber, the fare rates automatically increase when the taxi demand is higher than drivers around you. The Uber prices are surging to ensure reliability and availability for those who agree to pay a bit more.</p>

<h2 id="taxi-price-surge-use-case--3-key-architectural-questions">Taxi Price Surge Use Case – 3 Key Architectural Questions</h2>

<ul>
  <li>How do we handle the events like an ‘Order Request’ event or a ‘Pickup’ event?</li>
  <li>How do we compute the price accounting the nearby requests? We will need to find an efficient way to execute geospatial queries.</li>
  <li>How can we scale technology to run business in many cities, states or countries?</li>
</ul>

<h2 id="architecture">Architecture</h2>

<p>The following diagram illustrates the application architecture:</p>

<p><img src="/images/ie-geospatial/geo-demo-arch-diagram.jpg" /></p>

<h2 id="how-does-this-architecture-addresses-our-3-key-questions">How Does this Architecture Addresses Our 3 Key Questions?</h2>

<p>With InsightEdge Geospatial API we are able to efficiently find nearby orders and, therefore, minimize the time required to compute the price. The efficiency comes from the ability to index order request location in the datagrid.</p>

<p>Kafka allows to handle a high throughput of incoming raw events. Even if the computation layer starts processing slower(say during the peak hour), all the events will be reliably buffered in Kafka. The seamless and proven integration with Spark makes it a good choice for streaming applications.</p>

<p>InsightEdge Data Grid also plays a role of a serving layer handling any operational/transactional queries from web/mobile apps.</p>

<p>All the components(Kafka and InsightEdge) can scale out almost linearly;</p>

<p>To scale to many cities, we can leverage data locality principle through a full pipeline (Kafka, Spark, Data Grid) partitioning by the <code>city</code> or even with a more granular geographical units of scale. In this case the geospatial search query will be limited to a single Data Grid partition. We leave this enhancement out of the scope of the demo.</p>

<h2 id="building-a-demo-application-for-a-taxi-price-surge-use-case">Building a Demo Application for a Taxi Price Surge Use Case</h2>

<p>To simulate the taxi orders we took a <a href="https://github.com/fivethirtyeight/uber-tlc-foil-response">csv dataset</a> with Uber pickups in New York City. The demo application consists of following components:</p>

<ul>
  <li>Feeder application, reads the csv file and sends order and pickup events to Kafka</li>
  <li>InsightEdge processing, a Spark Streaming application that reads from Kafka, computes price and saves to datagrid</li>
  <li>Web app, reads orders from datagrid and visualizes them on a map</li>
</ul>

<p><img src="/images/ie-geospatial/demo_screenshot.jpg" /></p>

<h2 id="coding-processing-logic-with-insightedge-api">Coding Processing Logic with InsightEdge API</h2>

<p>Let’s see how InsightEdge API can be used to calculate the price:</p>

<p>&#8220;`scala
val ordersStream = initKafkaStream(ssc, “orders”) // step 1</p>

<p>ordersStream
  .map(message =&gt; Json.parse(message).as[OrderEvent]) // step 2
  .transform { rdd =&gt;  // step 3
    val query = “location spatial:within ? AND status = ?”
    val radius = 0.5 * DistanceUtils.KM_TO_DEG
    val queryParamsConstructor = (e: OrderEvent) =&gt; Seq(circle(point(e.longitude, e.latitude), radius), NewOrder)
    val projections = Some(Seq(“id”))
    rdd.zipWithGridSql<a href="query, queryParamsConstructor, projections">OrderRequest</a>
  }
  .map { case (e: OrderEvent, nearOrders: Seq[OrderRequest]) =&gt; // step 4
    val location = point(e.longitude, e.latitude)
    val nearOrderIds = nearOrders.map(_.id)
    val priceFactor = if (nearOrderIds.length &gt; 3) {
      1.0 + (nearOrderIds.length - 3) * 0.1
    } else {
      1.0
    }
    OrderRequest(e.id, e.time, location, priceFactor, nearOrderIds, NewOrder)
  }
  .saveToGrid() // step 5
&#8220;`</p>

<ul>
  <li>Step 1: Initialize a stream of Kafka <code>orders</code> topic</li>
  <li>Step 2: Parse Kafka message that is in Json format (in real app you may want to use formats like Avro)</li>
  <li>Step 3: For every order we find other non-processed orders within 0.3 km using InsightEdge’s <code>zipWithGridSql()</code> function</li>
  <li>Step 4: Given near orders, we calculate the price with a simple linear function</li>
  <li>Step 5: Finally we save the order details including price and near order ids into the data grid with <code>saveToGrid()</code>function</li>
</ul>

<p>The full source of the application is available on <a href="https://github.com/InsightEdge/insightedge-geo-demo">github</a></p>

<h2 id="taxi-price-surge-use-case-summary">Taxi Price Surge Use Case Summary</h2>

<p>In this blog post we created a demo application that processes the data stream using InsightEdge geospatial features.</p>

<p>An alternative approach for implementing dynamic price surging can use machine learning clustering algorithms to split order requests into clusters and calculate if the demand within a cluster is higher than the supply. This streaming application saves the cluster details in the datagrid. Then, to determine the price we execute a geospatial datagrid query to find which cluster the given location belongs to.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scalable machine learning with InsightEdge: mobile advertisement clicks prediction]]></title>
    <link href="http://dyagilev.org/blog/2016/05/30/scalable-machine-learning-with-insightedge-mobile-advertisement-clicks-prediction/"/>
    <updated>2016-05-30T20:34:42+03:00</updated>
    <id>http://dyagilev.org/blog/2016/05/30/scalable-machine-learning-with-insightedge-mobile-advertisement-clicks-prediction</id>
    <content type="html"><![CDATA[<p>This blog post will provide an introduction into using machine learning algorithms with <a href="http://insightedge.io/">InsightEdge</a>. We will go through an exercise to predict mobile advertisement click-through rate with <a href="https://www.kaggle.com/c/avazu-ctr-prediction">Avazu’s dataset</a>.</p>

<!-- more -->

<ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#loading-the-data" id="markdown-toc-loading-the-data">Loading the data</a></li>
  <li><a href="#exploring-the-data" id="markdown-toc-exploring-the-data">Exploring the data</a></li>
  <li><a href="#processing-and-transforming-the-data" id="markdown-toc-processing-and-transforming-the-data">Processing and transforming the data</a></li>
  <li><a href="#saving-preprocessed-data-to-the-data-grid" id="markdown-toc-saving-preprocessed-data-to-the-data-grid">Saving preprocessed data to the data grid</a></li>
  <li><a href="#a-simple-algorithm" id="markdown-toc-a-simple-algorithm">A simple algorithm</a></li>
  <li><a href="#experimenting-with-more-features" id="markdown-toc-experimenting-with-more-features">Experimenting with more features</a></li>
  <li><a href="#tuning-algorithm-parameters" id="markdown-toc-tuning-algorithm-parameters">Tuning algorithm parameters</a></li>
  <li><a href="#architecture" id="markdown-toc-architecture">Architecture</a></li>
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>There are several compensation models in online advertising industry, probably the most notable is CPC (Cost Per Click), in which an advertiser pays a publisher when the ad is clicked.
Search engine advertising is one of the most popular forms of CPC. It allows advertisers to bid for ad placement in a search engine’s sponsored links when someone searches on a keyword that is related to their business offering.</p>

<p>For the search engines like Google, advertising is one of the main sources of their revenue. The challenge for the advertising system is to determine what ad should be displayed for each query that the search engine receives.</p>

<p>The revenue search engine can get is essentially:</p>

<p><code>revenue = bid * probability_of_click</code></p>

<p>The goal is to maximize the revenue for every search engine query. Whereis the <code>bid</code> is a known value, the <code>probability_of_click</code> is not. Thus predicting the probability of click becomes the key task.</p>

<p>Working on a machine learning problem involves a lot of experiments with feature selection, feature transformation, training different models and tuning parameters.While there are a few excellent machine learning libraries for Python and R, like scikit-learn, their capabilities are typically limited to relatively small datasets that you fit on a single machine.</p>

<p>With the large datasets and/or CPU intensive workloads you may want to scale out beyond a single machine. This is one of the key benefits of InsightEdge, since it’s able to scale the computation and data storage layers across many machines under one unified cluster.</p>

<h2 id="loading-the-data">Loading the data</h2>

<p>The <a href="https://www.kaggle.com/c/avazu-ctr-prediction/data">dataset</a> consists of:</p>

<ul>
  <li>train (5.9G) - Training set. 10 days of click-through data, ordered chronologically. Non-clicks and clicks are subsampled according to different strategies.</li>
  <li>test (674M) - Test set. 1 day of ads to test model predictions.</li>
</ul>

<p>At first, we want to launch InsightEdge.</p>

<p>To get the first data insights quickly, one can <a href="http://insightedge.io/docs/010/0_quick_start.html">launch InsightEdge on a laptop</a>.
Though for the big datasets or compute-intensive tasks the resources of a single machine might not be enough.</p>

<p>For this problem we will <a href="http://insightedge.io/docs/010/13_cluster_setup.html">setup a cluster</a> with four workers and place the downloaded files on HDFS.</p>

<p><img src="/images/ie-ml/0_cluster.png" /></p>

<p>Let’s open the interactive <a href="http://insightedge.io/docs/010/14_notebook.html">Web Notebook</a> and start exploring our dataset.</p>

<p>The dataset is in csv format, so we will use databricks csv library to load it from hdfs into the Spark dataframe:</p>

<p><code>scala
%dep
z.load("com.databricks:spark-csv_2.10:1.3.0")
</code></p>

<p>Load the dataframe into Spark memory and cache:</p>

<p>&#8220;`scala
val df = sqlContext.read
      .format(“com.databricks.spark.csv”)
      .option(“header”, “true”)
      .option(“inferSchema”, “false”)
      .load(“hdfs://10.8.1.116/data/avazu_ctr/train”)</p>

<p>df.cache()
&#8220;`</p>

<h2 id="exploring-the-data">Exploring the data</h2>

<p>Now that we have the dataset in Spark memory, we can read the first rows:</p>

<p><code>scala
df.show(10)
</code></p>

<p><code>
+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+
|                  id|click|    hour|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|
+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+
| 1000009418151094273|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| ddd2926e|    44956a24|          1|               2|15706|320| 50|1722|  0| 35|    -1| 79|
|10000169349117863715|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 96809ac8|    711ee120|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|
|10000371904215119486|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| b3cf8def|    8a4875bd|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|
|10000640724480838376|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| e8275b8f|    6332421a|          1|               0|15706|320| 50|1722|  0| 35|100084| 79|
|10000679056417042096|    0|14102100|1005|         1|fe8cc448|   9166c161|     0569f928|ecad2386|  7801e8d9|    07d7df22| a99f214a| 9644d0bf|    779d90c2|          1|               0|18993|320| 50|2161|  0| 35|    -1|157|
|10000720757801103869|    0|14102100|1005|         0|d6137915|   bb1ef334|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| 05241af0|    8a4875bd|          1|               0|16920|320| 50|1899|  0|431|100077|117|
|10000724729988544911|    0|14102100|1005|         0|8fda644b|   25d4cfcd|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| b264c159|    be6db1d7|          1|               0|20362|320| 50|2333|  0| 39|    -1|157|
|10000918755742328737|    0|14102100|1005|         1|e151e245|   7e091613|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| e6f67278|    be74e6fe|          1|               0|20632|320| 50|2374|  3| 39|    -1| 23|
|10000949271186029916|    1|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 37e8da74|    5db079b5|          1|               2|15707|320| 50|1722|  0| 35|    -1| 79|
|10001264480619467364|    0|14102100|1002|         0|84c7ba46|   c4e18dd6|     50e219e0|ecad2386|  7801e8d9|    07d7df22| c357dbff| f1ac7184|    373ecbe6|          0|               0|21689|320| 50|2496|  3|167|100191| 23|
+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+
only showing top 10 rows
</code></p>

<p>The data fields are:</p>

<ul>
  <li>id: ad identifier</li>
  <li>click: 0/1 for non-click/click</li>
  <li>hour: format is YYMMDDHH</li>
  <li>C1: anonymized categorical variable</li>
  <li>banner_pos</li>
  <li>site_id</li>
  <li>site_domain</li>
  <li>site_category</li>
  <li>app_id</li>
  <li>app_domain</li>
  <li>app_category</li>
  <li>device_id</li>
  <li>device_ip</li>
  <li>device_model</li>
  <li>device_type</li>
  <li>device_conn_type</li>
  <li>C14-C21 – anonymized categorical variables</li>
</ul>

<p>Let’s see how many rows are in the training dataset:</p>

<p>&#8220;`scala
val totalCount = df.count()</p>

<p>totalCount: Long = 40428967
&#8220;`</p>

<p>There are about 40M+ rows in the dataset.</p>

<p>Let’s now calculate the CTR (click-through rate) of the dataset. The click-through rate is the number of times a click is made on the advertisement divided by the total impressions (the number of times an advertisement was served):</p>

<p>&#8220;`scala
val clicks = df.filter(“click = 1”).count()
val ctr = clicks.toFloat / totalCount</p>

<p>clicks: Long = 6865066
ctr: Float = 0.16980562
&#8220;`
The CTR is 0.169 (or 16.9%) which is quite a high number, the common value in the industry is about 0.2-0.3%. So a high value is probably because non-clicks and clicks are subsampled according to different strategies, as stated by Avazu.</p>

<p>Now, the question is which features should we use to create a predictive model? This is a difficult question that requires a deep knowledge of the problem domain. Let’s try to learn it from the dataset we have.</p>

<p>For example, let’s explore the <code>device_conn_type</code> feature. Our assumption might be that this is a categorical variable like Wi-Fi, 2G, 3G or LTE. This might be a relevant feature since clicking on an ad with a slow connection is not something common.</p>

<p>At first, we register the dataframe as a SQL table:</p>

<p><code>scala
df.registerTempTable("training")
</code></p>

<p>and run the SQL query:</p>

<p><code>sql
%sql
SELECT device_conn_type, SUM(click) as clicks_num, COUNT(click) as impression, SUM(click)/COUNT(click) as ctr
FROM training
GROUP BY device_conn_type
</code></p>

<p><img src="/images/ie-ml/6_device_conn_type.png" /></p>

<p><img src="/images/ie-ml/7_device_conn_type_2.png" /></p>

<p>We see that there are four connection type categories. Two categories with CTR 18% and 13%, and the first one is almost 90% of the whole dataset. The other two categories have significantly lower CTR.</p>

<p>Another observation we may notice is that features C15 and C16 look like the ad size:</p>

<p><code>sql
%sql
SELECT C15, C16, COUNT(click) as impression, SUM(click)/COUNT(click) as ctr
FROM training
GROUP BY C15, C16
ORDER BY ctr DESC
</code></p>

<p><img src="/images/ie-ml/11_banner_dimension.png" /></p>

<p>We can notice some correlation between the ad size and its performance. The most common one appears to be 320x50px known as “mobile leaderboard” in <a href="https://support.google.com/adsense/answer/68727?hl=en">Google AdSense</a>.</p>

<p>What about other features? All of them represent categorical values, how many unique categories for each feature?</p>

<p>&#8220;`scala
df.columns.map(c =&gt; (c, df.select(c).distinct().count()))</p>

<p>res14: Array[(String, Long)] = Array((id,40428967), (click,2), (hour,240), (C1,7), (banner_pos,7), (site_id,4737), (site_domain,7745), (site_category,26), (app_id,8552), (app_domain,559), (app_category,36), (device_id,2686408), (device_ip,6729486), (device_model,8251), (device_type,5), (device_conn_type,4), (C14,2626), (C15,8), (C16,9), (C17,435), (C18,4), (C19,68), (C20,172), (C21,60))
&#8220;`</p>

<p>We see that there are some features with a lot of unique values, for example, <code>device_ip</code> has 6M+ different values.
Machine learning algorithms are typically defined in terms of numerical vectors rather than categorical values. Converting such categorical features will result in a high dimensional vector which might be very expensive.
We will need to deal with this later.</p>

<h2 id="processing-and-transforming-the-data">Processing and transforming the data</h2>

<p>Looking further at the dataset, we can see that the <code>hour</code> feature is in <code>YYMMDDHH</code> format.
To allow the predictive model to effectively learn from this feature it makes sense to transform it into three features: year, month and hour.
Let’s develop the function to transform the dataframe:</p>

<p>&#8220;`scala
import java.text.SimpleDateFormat
import java.util.{Calendar, Date}
import org.apache.spark.sql.DataFrame</p>

<p>object DateUtils {
  val dateFormat = new ThreadLocal<a href="">SimpleDateFormat</a> {
    override def initialValue(): SimpleDateFormat = new SimpleDateFormat(“yyMMddHH”)
  }</p>

<p>def parse(s: String, field: Int): Int = {
    val date = dateFormat.get().parse(s)
    val cal = Calendar.getInstance()
    cal.setTime(date)
    cal.get(field)
  }
}</p>

<p>def transformHour(df: DataFrame): DataFrame = {
  val toYear = udf<a href="s =&gt; DateUtils.parse(s, Calendar.YEAR)">Int, String</a>
  val toMonth = udf<a href="s =&gt; DateUtils.parse(s, Calendar.MONTH)">Int, String</a>
  val toDay = udf<a href="s =&gt; DateUtils.parse(s, Calendar.DAY_OF_MONTH)">Int, String</a>
  val toHour = udf<a href="s =&gt; DateUtils.parse(s, Calendar.HOUR_OF_DAY)">Int, String</a></p>

<p>df.withColumn(“time_year”, toYear(df(“hour”)))
  .withColumn(“time_month”, toMonth(df(“hour”)))
  .withColumn(“time_day”, toDay(df(“hour”)))
  .withColumn(“time_hour”, toHour(df(“hour”)))
  .drop(“hour”)
}
&#8220;`</p>

<p>We can now apply this transformation to our dataframe and see the result:</p>

<p><code>scala
val hourDecoded = transformHour(df)
hourDecoded.cache()
hourDecoded.show(10)
</code></p>

<p><code>
+--------------------+-----+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+---------+----------+--------+---------+
|                  id|click|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|time_year|time_month|time_day|time_hour|
+--------------------+-----+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+---------+----------+--------+---------+
| 1000009418151094273|    0|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| ddd2926e|    44956a24|          1|               2|15706|320| 50|1722|  0| 35|    -1| 79|     2014|         9|      21|        0|
|10000169349117863715|    0|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 96809ac8|    711ee120|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|     2014|         9|      21|        0|
|10000371904215119486|    0|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| b3cf8def|    8a4875bd|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|     2014|         9|      21|        0|
|10000640724480838376|    0|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| e8275b8f|    6332421a|          1|               0|15706|320| 50|1722|  0| 35|100084| 79|     2014|         9|      21|        0|
|10000679056417042096|    0|1005|         1|fe8cc448|   9166c161|     0569f928|ecad2386|  7801e8d9|    07d7df22| a99f214a| 9644d0bf|    779d90c2|          1|               0|18993|320| 50|2161|  0| 35|    -1|157|     2014|         9|      21|        0|
|10000720757801103869|    0|1005|         0|d6137915|   bb1ef334|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| 05241af0|    8a4875bd|          1|               0|16920|320| 50|1899|  0|431|100077|117|     2014|         9|      21|        0|
|10000724729988544911|    0|1005|         0|8fda644b|   25d4cfcd|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| b264c159|    be6db1d7|          1|               0|20362|320| 50|2333|  0| 39|    -1|157|     2014|         9|      21|        0|
|10000918755742328737|    0|1005|         1|e151e245|   7e091613|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| e6f67278|    be74e6fe|          1|               0|20632|320| 50|2374|  3| 39|    -1| 23|     2014|         9|      21|        0|
|10000949271186029916|    1|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 37e8da74|    5db079b5|          1|               2|15707|320| 50|1722|  0| 35|    -1| 79|     2014|         9|      21|        0|
|10001264480619467364|    0|1002|         0|84c7ba46|   c4e18dd6|     50e219e0|ecad2386|  7801e8d9|    07d7df22| c357dbff| f1ac7184|    373ecbe6|          0|               0|21689|320| 50|2496|  3|167|100191| 23|     2014|         9|      21|        0|
+--------------------+-----+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+---------+----------+--------+---------+
</code></p>

<p>It looks like the year and month have only one value, let’s verify it:</p>

<p>&#8220;`scala
hourDecoded.select(“time_month”).distinct.count()
hourDecoded.select(“time_year”).distinct.count()</p>

<p>res20: Long = 1
res21: Long = 1
&#8220;`</p>

<p>We can safely drop these columns as they don’t bring any knowledge to our model:</p>

<p><code>scala
val hourDecoded2 = hourDecoded.drop("time_month").drop("time_year")
</code></p>

<p>Let’s also convert <code>click</code> from String to Double type.</p>

<p>&#8220;`scala
import org.apache.spark.sql.types.DoubleType</p>

<p>val prepared = hourDecoded2
    .withColumn(“clickTmp”, hourDecoded2(“click”).cast(DoubleType))
    .drop(“click”)
    .withColumnRenamed(“clickTmp”, “click”)
&#8220;`</p>

<h2 id="saving-preprocessed-data-to-the-data-grid">Saving preprocessed data to the data grid</h2>

<p>The entire training dataset contains 40M+ rows, it takes quite a long time to experiment with different algorithms and approaches even in a clustered environment.
We want to sample the dataset and checkpoint it to the in-memory data grid that is running collocated with Spark.
This way we can:
* quickly iterate through different approaches
* restart the Zeppelin session or launch other Spark applications and pick up the dataset more quickly from memory</p>

<p>Since the training dataset contains the data for the 10 days, we can pick any day and sample it:</p>

<p>&#8220;`scala
prepared.filter(“time_day = 21”).count()</p>

<p>res51: Long = 4122995
&#8220;`</p>

<p>There are 4M+ rows for this day, which is about 10% of the entire dataset.</p>

<p>Now let’s save it to the data grid. This can be done with two lines of code:</p>

<p><code>scala
import org.apache.spark.sql.insightedge._
prepared.filter("time_day = 21").write.grid.save("day_21")
</code></p>

<p>Any time later in another Spark context we can bring the collection to the Spark memory with:
<code>scala
val df = sqlContext.read.grid.load("day_21")
</code></p>

<p>Also, we want to transform the <code>test</code> dataset that we will use for prediction in a similar way.</p>

<p>&#8220;`scala
val testDf = sqlContext.read
      .format(“com.databricks.spark.csv”)
      .option(“header”, “true”)
      .option(“inferSchema”, “false”)
      .load(“hdfs://10.8.1.116/data/avazu_ctr/test”)</p>

<p>transformHour(testDf)
    .drop(“time_month”)
    .drop(“time_year”)
    .write.grid.save(“test”)</p>

<p>&#8220;`</p>

<p>The complete listing of notebook can be found on <a href="https://github.com/InsightEdge/insightedge-ctr-demo/blob/master/zeppelin/CTR%20demo.json">github</a>. You can import it to Zeppelin and play with it on your own.</p>

<h2 id="a-simple-algorithm">A simple algorithm</h2>

<p>Now that we have training and test datasets sampled, initially preprocessed and available in the data grid, we can close Web Notebook and start experimenting with
different techniques and algorithms by submitting Spark applications.</p>

<p>For our first baseline approach let’s take a single feature <code>device_conn_type</code> and <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> algorithm:</p>

<p>&#8220;`scala
import com.gigaspaces.spark.context.GigaSpacesConfig
import com.gigaspaces.spark.implicits._
import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.insightedge._
import org.apache.spark.{SparkConf, SparkContext}</p>

<p>object CtrDemo1 {</p>

<p>def main(args: Array[String]): Unit = {
    if (args.length &lt; 3) {
      System.err.println(“Usage: CtrDemo1 <spark master="" url=""> <grid locator=""> <train collection="">&#8221;)
      System.exit(1)
    }</train></grid></spark></p>

<pre><code>val Array(master, gridLocator, trainCollection) = args

// Configure InsightEdge settings
val gsConfig = GigaSpacesConfig("insightedge-space", None, Some(gridLocator))
val sc = new SparkContext(new SparkConf().setAppName("CtrDemo1").setMaster(master).setGigaSpaceConfig(gsConfig))
val sqlContext = new SQLContext(sc)

// load training collection from data grid
val trainDf = sqlContext.read.grid.load(trainCollection)
trainDf.cache()

// use one-hot-encoder to convert 'device_conn_type' categorical feature into a vector
val indexed = new StringIndexer()
  .setInputCol("device_conn_type")
  .setOutputCol("device_conn_type_index")
  .fit(trainDf)
  .transform(trainDf)

val encodedDf = new OneHotEncoder()
  .setDropLast(false)
  .setInputCol("device_conn_type_index")
  .setOutputCol("device_conn_type_vector")
  .transform(indexed)

// convert dataframe to a label points RDD
val encodedRdd = encodedDf.map { row =&gt;
  val label = row.getAs[Double]("click")
  val features = row.getAs[Vector]("device_conn_type_vector")
  LabeledPoint(label, features)
}

// Split data into training (60%) and test (40%)
val Array(trainingRdd, testRdd) = encodedRdd.randomSplit(Array(0.6, 0.4), seed = 11L)
trainingRdd.cache()

// Run training algorithm to build the model
val model = new LogisticRegressionWithLBFGS()
  .setNumClasses(2)
  .run(trainingRdd)

// Clear the prediction threshold so the model will return probabilities
model.clearThreshold

// Compute raw scores on the test set
val predictionAndLabels = testRdd.map { case LabeledPoint(label, features) =&gt;
  val prediction = model.predict(features)
  (prediction, label)
}

// Instantiate metrics object
val metrics = new BinaryClassificationMetrics(predictionAndLabels)

val auROC = metrics.areaUnderROC
println("Area under ROC = " + auROC)   } } ```
</code></pre>

<p>We will explain a little bit more what happens here.</p>

<p>At first, we load the training dataset from the data grid, which we prepared and saved earlier with Web Notebook.</p>

<p>Then we use <a href="https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/ml/feature/StringIndexer.html">StringIndexer</a> and <a href="https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/ml/feature/OneHotEncoder.html">OneHotEncoder</a> to map  a column of categories to a column of binary vectors. For example, with 4 categories of <code>device_conn_type</code>, an input value
of the second category would map to an output vector of <code>[0.0, 1.0, 0.0, 0.0, 0.0]</code>.</p>

<p>Then we convert a dataframe to an <code>RDD[LabeledPoint]</code> since the <a href="https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/mllib/classification/LogisticRegressionWithLBFGS.html">LogisticRegressionWithLBFGS</a> expects RDD as a training parameter.
We train the logistic regression and use it to predict the click for the test dataset. Finally we compute the metrics of our classifier comparing the predicted labels with actual ones.</p>

<p>To build this application and submit it to the InsightEdge cluster:
<code>bash
sbt clean assembly
./bin/insightedge-submit --class io.insightedge.demo.ctr.CtrDemo1 --master spark://10.8.1.115:7077 --executor-memory 16G  ~/avazu_ctr/insightedge-ctr-demo-assembly-1.0.0.jar spark://10.8.1.115:7077 10.8.1.115:4174 day_21
</code></p>

<p>It takes about 2 minutes for the application to complete and output the following:
<code>
Area under ROC = 0.5177127622153417
</code></p>

<p>We get <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">AUROC</a> slightly better than a random guess (AUROC = 0.5), which is not so bad for our first approach, but we can definitely do better.</p>

<h2 id="experimenting-with-more-features">Experimenting with more features</h2>

<p>Let’s try to select more features and see how it affects our metrics.</p>

<p>For this we created a new version of our app <a href="https://github.com/InsightEdge/insightedge-ctr-demo/blob/master/src/main/scala/io/insightedge/demo/ctr/CtrDemo2.scala">CtrDemo2</a> where we
can easily select features we want to include. We use <a href="https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/ml/feature/VectorAssembler.html">VectorAssembler</a> to assemble multiple feature vectors into a single <code>features</code> one:</p>

<p><code>scala
val assembledDf = new VectorAssembler()
  .setInputCols(categoricalColumnsVectors.toArray)
  .setOutputCol("features")
  .transform(encodedDf)
</code></p>

<p>The results are the following:</p>

<ul>
  <li>with additionally included <code>device_type</code>: AUROC = 0.531015564807053</li>
  <li><code>+</code> <code>time_day</code> and <code>time_hour</code>: AUROC = 0.5555488992624483</li>
  <li><code>+</code> <code>C15</code>, <code>C16</code>, <code>C17</code>, <code>C18</code>, <code>C19</code>, <code>C20</code>, <code>C21</code>: AUROC = 0.7000630113145946</li>
</ul>

<p>You can notice how the AUROC is being improved as we add more and more features. This comes with the cost of the training time:</p>

<p><img src="/images/ie-ml/13_application_time.png" /></p>

<p>We didn’t include high-cardinality features such as <code>device_ip</code> and <code>device_id</code> as they will blow up the feature vector size. One may consider applying techniques such as feature hashing
to reduce the dimension. We will leave it out of this blog post’s scope.</p>

<h2 id="tuning-algorithm-parameters">Tuning algorithm parameters</h2>

<p>Tuning algorithm parameters is a search problem. We will use <a href="http://spark.apache.org/docs/latest/ml-guide.html">Spark Pipeline API</a> with a <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">Grid Search</a> technique.
Grid search evaluates a model for each combination of algorithm parameters specified in a grid (do not confuse with data grid).</p>

<p>Pipeline API supports model selection using <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a> technique. For each set of parameters it trains the given <code>Estimator</code> and evaluates it using the given <code>Evaluator</code>.
We will use <code>BinaryClassificationEvaluator</code> that has AUROC as a metric by default.</p>

<p>&#8220;`scala
val lr = new LogisticRegression().setLabelCol(“click”)
val pipeline = new Pipeline().setStages(Array(assembler, lr))</p>

<p>val paramGrid = new ParamGridBuilder()
  .addGrid(lr.regParam, Array(0.01 , 0.1 /<em>, 1.0 */))
  .addGrid(lr.elasticNetParam, Array(0.0 /</em>, 0.5 , 1.0 <em>/))
  .addGrid(lr.fitIntercept, Array(false /</em>, true */))
  .build()</p>

<p>val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(new BinaryClassificationEvaluator().setLabelCol(“click”))
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)</p>

<p>val cvModel = cv.fit(encodedTrainDf)
&#8220;`</p>

<p>We included two regularization parameters 0.01 and 0.1 in our search grid for now, others are commented out for now.</p>

<p>Output the best set of parameters:</p>

<p>&#8220;`scala
println(“Grid search results:”)
cvModel.getEstimatorParamMaps.zip(cvModel.avgMetrics).foreach(println)</p>

<p>println(“Best set of parameters found:” + cvModel.getEstimatorParamMaps
  .zip(cvModel.avgMetrics)
  .maxBy(_._2)
  ._1)
&#8220;`</p>

<p>Use the best model to predict <code>test</code> dataset loaded from the data grid:
<code>scala
val predictionDf = cvModel.transform(encodedTestDf).select("id", "probability").map {
  case Row(id: String, probability: Vector) =&gt; (id, probability(1))
}.toDF("id", "click")
</code></p>

<p>Then the results are saved back to csv on hdfs, so we can submit them to Kaggle, see the complete listing in <a href="https://github.com/InsightEdge/insightedge-ctr-demo/blob/master/src/main/scala/io/insightedge/demo/ctr/CtrDemo3.scala">CtrDemo3</a>.</p>

<p>It takes about 27 mins to train and compare models for two regularization parameters 0.01 and 0.1. The results are:
&#8220;`
Grid search results:
({
	logreg_2b70e3edf1f0-elasticNetParam: 0.0,
	logreg_2b70e3edf1f0-fitIntercept: false,
	logreg_2b70e3edf1f0-regParam: 0.01
},0.7000655195682837)
({
	logreg_2b70e3edf1f0-elasticNetParam: 0.0,
	logreg_2b70e3edf1f0-fitIntercept: false,
	logreg_2b70e3edf1f0-regParam: 0.1
},0.697680175815199)</p>

<p>Best params found:{
	logreg_2b70e3edf1f0-elasticNetParam: 0.0,
	logreg_2b70e3edf1f0-fitIntercept: false,
	logreg_2b70e3edf1f0-regParam: 0.01
}
&#8220;`</p>

<p>This simple logistic regression model has a rank of 1109 out of 1603 competitors in Kaggle.</p>

<p>The future improvements are only limited by data science skills and creativity. One may consider:</p>

<ul>
  <li>implement <a href="https://www.kaggle.com/wiki/LogarithmicLoss">Logarithmic Loss</a> function as an Evaluator since it’s used by Kaggle to calculate the model score. In our example we used AUROC</li>
  <li>include other features that we didn’t select</li>
  <li>generate additional features such click history of a user</li>
  <li>use a hashing trick to reduce the features vector dimension</li>
  <li>try other machine learning algorithms, the winner of competition used  <a href="http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf">Field-aware Factorization Machines</a></li>
</ul>

<h2 id="architecture">Architecture</h2>

<p>The following diagram demonstrates the design of machine learning application with InsightEdge.</p>

<p><img src="/images/ie-ml/arch.png" /></p>

<p>The key design advantages are:</p>

<ul>
  <li>the single platform <strong>converges</strong> analytical processing (machine learning) powered by Spark with transactional processing powered by custom real-time applications;</li>
  <li>real-time applications can execute any OLTP query (read, insert, update, delete) on training data that is <strong>immediately available</strong> for Spark analytical queries or machine learning routines. There is no need to build a complex ETL pipeline that extracts training data from OLTP database with Kafka/Flume/HDFS. Besides the complexity, an ETL pipeline introduces unwanted latency that can be a stopper for reactive machine learning apps.
With InsightEdge, Spark applications can view the <strong>live</strong> data;</li>
  <li>the training data lives in the memory of data grid, which acts as an extension of Spark memory. This way we can load the data <strong>quicker</strong>;</li>
  <li>An in-memory data grid is a <strong>general-purpose</strong> highly available and fault tolerant storage. With support of ACID transactions and SQL queries it becomes the primary storage for the application;</li>
  <li>InsightEdge stack is <strong>scalable</strong> in both computation (Spark) and storage (data grid) tiers. This makes it attractive for large-scale machine learning.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>In this blog post we demonstrated how to use machine learning algorithms with InsightEdge. We went through typical stages:</p>

<ul>
  <li>interactive data exploration with Zeppelin</li>
  <li>feature selection and transformation</li>
  <li>training predictive models</li>
  <li>calculating model metrics</li>
  <li>tuning parameters</li>
</ul>

<p>We didn’t have a goal to build a perfect predictive model, so there is great room for improvement.</p>

<p>In the architecture section we discussed how the typical design may look like, what are the benefits of using InsightEdge for machine learning.</p>

<p>The Zeppelin notebook can be found <a href="https://github.com/InsightEdge/insightedge-ctr-demo/blob/master/zeppelin/CTR%20demo.json">here</a> and submittable spark apps <a href="https://github.com/InsightEdge/insightedge-ctr-demo/tree/master/src/main/scala/io/insightedge/demo/ctr">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keep up with Spark Streaming at in-memory speed using GigaSpaces XAP]]></title>
    <link href="http://dyagilev.org/blog/2015/03/07/Keep-up-with-Spark-Streaming-at-in-memory-speed-using-GigaSpaces-XAP/"/>
    <updated>2015-03-07T21:33:06+02:00</updated>
    <id>http://dyagilev.org/blog/2015/03/07/Keep-up-with-Spark-Streaming-at-in-memory-speed-using-GigaSpaces-XAP</id>
    <content type="html"><![CDATA[<p>Spark Streaming is a popular engine for stream processing and its ability to compute data in memory makes it very attractive. However Spark Streaming is not self-sufficient, it relies on external data source and storage to output computation results. Therefore, in many cases the overall performance is limited by slow external components that are not able to keep up with Spark’s throughput and/or introduce unacceptable latency.</p>

<p>In this article we describe how we use GigaSpaces XAP in-memory datagrid to address this challenge. Code sources are available on <a href="https://github.com/fe2s/xap-spark">github</a></p>

<!-- more -->

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#challenge" id="markdown-toc-challenge">Challenge</a></li>
  <li><a href="#solution" id="markdown-toc-solution">Solution</a>    <ul>
      <li><a href="#xap-stream" id="markdown-toc-xap-stream">XAP Stream</a></li>
      <li><a href="#spark-input-dstream" id="markdown-toc-spark-input-dstream">Spark Input DStream</a></li>
      <li><a href="#output-spark-computation-results-to-xap" id="markdown-toc-output-spark-computation-results-to-xap">Output Spark computation results to XAP</a></li>
    </ul>
  </li>
  <li><a href="#word-counter-demo" id="markdown-toc-word-counter-demo">Word Counter Demo</a>    <ul>
      <li><a href="#high-level-design" id="markdown-toc-high-level-design">High-level design</a></li>
      <li><a href="#installing-and-building-the-demo-application" id="markdown-toc-installing-and-building-the-demo-application">Installing and building the Demo application</a></li>
      <li><a href="#deploying-xap-space-and-web-pu" id="markdown-toc-deploying-xap-space-and-web-pu">Deploying XAP Space and Web PU</a></li>
      <li><a href="#launch-spark-application" id="markdown-toc-launch-spark-application">Launch Spark Application</a>        <ul>
          <li><a href="#option-a-run-embedded-spark-cluster" id="markdown-toc-option-a-run-embedded-spark-cluster">Option A. Run embedded Spark cluster</a></li>
          <li><a href="#option-b-run-spark-standalone-mode-cluster" id="markdown-toc-option-b-run-spark-standalone-mode-cluster">Option B. Run Spark standalone mode cluster</a>            <ul>
              <li><a href="#run-spark" id="markdown-toc-run-spark">Run Spark</a></li>
              <li><a href="#submit-application" id="markdown-toc-submit-application">Submit application</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#launch-feeder-application" id="markdown-toc-launch-feeder-application">Launch Feeder application</a></li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>Real-time processing is becoming more and more popular. <a href="https://spark.apache.org/streaming/">Spark Streaming</a> is an extension of the core Spark API that allows scalable, high-throughput, fault-tolerant stream processing of live data streams.</p>

<p>Spark Streaming has many use cases: user activity analytics on web, recommendation systems, censor data analytics, fraud detection, sentiment analytics and more.</p>

<p>Data can be ingested to Spark cluster from many sources like HDS, Kafka, Flume, etc and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>. Finally, processed data can be pushed out to filesystems or databases.</p>

<p><img src="/images/xap-spark/spark-streaming.jpg" /></p>

<h1 id="challenge">Challenge</h1>

<p>Spark cluster keeps intermediate chunks of data (RDD) in memory and, if required, rarely touches HDFS to checkpoint stateful computation, therefore it is able to process huge volumes of data at in-memory speed. However, in many cases the overall performance is limited by slow input and output data sources that are not able to stream and store data with in-memory speed.</p>

<h1 id="solution">Solution</h1>

<p>In this pattern we address performance challenge by integrating Spark Streaming with XAP. XAP is used as a stream data source and a scalable, fast, reliable persistent storage.</p>

<p><img src="/images/xap-spark/high-level.jpg" /></p>

<ol>
  <li>Producer writes the data to XAP stream</li>
  <li>Spark worker reads the data from XAP stream and propagates it further for computation</li>
  <li>Spark saves computation results to XAP datagrid where they can be queried to display on UI</li>
</ol>

<p>Let’s discuss this in more details.</p>

<h2 id="xap-stream">XAP Stream</h2>

<p>On XAP side we introduce the concept of stream. Please find <code>XAPStream</code> – an implementation that supports writing data in single and batch modes and reading in batch mode. <code>XAPStream</code> leverages XAP’s <code>FIFO</code> (First In, First Out) capabilities.</p>

<p>Here is an example how one can write data to <code>XAPStream</code>. Let’s consider we are building a Word Counter application and would like to write sentences of text to the stream.</p>

<p>At first we create a data model that represents a sentence. Note, that the space class should be annotated with <code>FIFO</code> support.</p>

<p><code>java
@SpaceClass(fifoSupport = FifoSupport.OPERATION)
public class Sentence implements Serializable {
    private String id;
    private String text;
    [getters setters omitted for brevity]
}
</code>
&gt; Complete sources of Sentence.java can be found <a href="https://github.com/fe2s/xap-spark/blob/master/word-counter-demo/space-model/src/main/java/com/gigaspaces/spark/streaming/wordcounter/Sentence.java">here</a></p>

<h2 id="spark-input-dstream">Spark Input DStream</h2>

<p>In order to ingest data from XAP to Spark, we implemented a custom <code>ReceiverInputDStream</code> that starts the <code>XAPReceiver</code> on Spark worker nodes to receive the data.</p>

<p><code>XAPReceiver</code> is a stream consumer that reads batches of data in multiple threads in parallel to achieve the maximum throughput.</p>

<p><code>XAPInputDStream</code> can be created using the following function in <code>XAPUtils</code> object.</p>

<p><code>scala
/**
   * Creates InputDStream with GigaSpaces XAP used as an external data store
   *
   * @param ssc streaming context
   * @param storageLevel RDD persistence level
   * @param template template used to match items when reading from XAP stream
   * @param batchSize number of items to read from
   * @param readRetryInterval time to wait till the next read attempt if nothing consumed
   * @param parallelReaders number of parallel readers
   * @tparam T Class type of the object of this stream
   * @return Input DStream
   */
  def createStream[T &lt;: java.io.Serializable : ClassTag](ssc: StreamingContext, template: T, batchSize:Int, readRetryInterval: Duration = Milliseconds(100), parallelReaders: Int, storageLevel: StorageLevel = MEMORY_AND_DISK_SER){…}
</code></p>

<p>Here is an example of creating XAP Input stream. At first we set XAP space url in Spark config:</p>

<p><code>scala
  val sparkConf = new SparkConf()
      .setAppName("XAPWordCount")
      .setMaster(config.sparkMaster)
      .setJars(Seq(config.jarLocation))
      .set(XAPUtils.SPACE_URL_CONF_KEY, "jini://*/*/space")
</code></p>

<p>And then we create a stream by merging two parallel sub-streams:</p>

<p><code>scala
val numStreams = 2
val streams = (1 to numStreams).map(_ =&gt; XAPUtils.createStream(context, new Sentence(), 50, Milliseconds(100), 4))
val stream = context.union(streams)
</code></p>

<p>Once the stream is created, we can apply any Spark functions like <code>map</code>, <code>filter</code>, <code>reduce</code>, <code>transform</code>, etc.</p>

<p>For instance, to compute a word counter of five-letter words over a sliding window, one can do the following:</p>

<p><code>scala
val words = stream.flatMap(_.getText.split(" ")).filter(_.size == 5)
val wordCountWindow = words
      .map((_, 1))
      .reduceByKeyAndWindow(_ + _, Seconds(5))
      .map { case (word, count) =&gt; (count, word)}
      .transform(_.sortByKey(ascending = false))
</code></p>

<h2 id="output-spark-computation-results-to-xap">Output Spark computation results to XAP</h2>

<p>Output operations allow the <code>DStream</code>’s data to be pushed out to external systems. Please refer to <a href="https://spark.apache.org/docs/1.1.0/streaming-programming-guide.html#output-operations-on-dstreams">Spark documentation</a> for the details.</p>

<p>To minimize the cost of creating XAP connection for each <code>RDD</code>, we created a connection pool named <code>GigaSpaceFactory</code>. Here is an example how to output <code>RDD</code> to XAP:</p>

<p><code>scala
wordCountWindow.foreachRDD(rdd =&gt; {
      val gigaSpace = GigaSpaceFactory.getOrCreate(config.spaceUrl)
      val topList = rdd.take(10).map { case (count, word) =&gt; new WordCount(word, count)}
      gigaSpace.write(new TopWordCounts(topList))
})
</code></p>

<blockquote>
  <p>Please, note that in this example a XAP connection is created and data is written from Spark driver. In some cases, one may want to write data from the Spark worker. Please, refer to Spark documentation - it explains different design patterns using <code>foreachRDD</code>.</p>
</blockquote>

<h1 id="word-counter-demo">Word Counter Demo</h1>

<p>As a part of this integration pattern, we demonstrate how to build an application that consumes live stream of text and displays top 10 five-letter words over a sliding window in real-time. The user interface consists of a simple single page web application displaying a table of top 10 words and a word cloud. The data on UI is updated every second.</p>

<p><img src="/images/xap-spark/spark-word-counter.jpg" /></p>

<h2 id="high-level-design">High-level design</h2>

<p>The high-level design diagram of the Word Counter Demo is below:</p>

<p><img src="/images/xap-spark/example.jpg" /></p>

<ol>
  <li>Feeder is a standalone scala application that reads book from text file in a cycle and writes lines to XAP Stream.</li>
  <li>Stream is consumed by the Spark cluster which performs all necessary computing.</li>
  <li>Computation results are stored in the XAP space.</li>
  <li>End user is browsing the web page hosted in a Web PU that continuously updates dashboard with AJAX requests backed by the rest service.</li>
</ol>

<h2 id="installing-and-building-the-demo-application">Installing and building the Demo application</h2>

<ol>
  <li>Download <a href="http://www.gigaspaces.com/LatestProductVersion">XAP</a></li>
  <li><a href="http://docs.gigaspaces.com/xap100/installation.html">Install XAP</a></li>
  <li>Install Maven and the <a href="http://docs.gigaspaces.com/xap100/maven-plugin.html">XAP Maven plug-in</a></li>
  <li>Download the application <a href="https://github.com/fe2s/xap-spark">source code</a></li>
  <li>Build a project by running <code>mvn clean install</code></li>
</ol>

<h2 id="deploying-xap-space-and-web-pu">Deploying XAP Space and Web PU</h2>

<ol>
  <li>Set the XAP lookup group to <code>spark</code> by adding <code>export LOOKUPGROUPS=spark</code> line to <code>&lt;XAP_HOME&gt;/bin/setenv.sh/bat</code></li>
  <li>Start a Grid Service Agent by running the <code>gs-agent.sh/bat</code> script</li>
  <li>Deploy a space by running <code>mvn os:deploy -Dgroups=spark</code> from <code>&lt;project_root&gt;/word-counter-demo</code> directory</li>
</ol>

<h2 id="launch-spark-application">Launch Spark Application</h2>

<h3 id="option-a-run-embedded-spark-cluster">Option A. Run embedded Spark cluster</h3>

<p>This is the simplest option that doesn’t require downloading and installing Spark distributive, which is useful for the development purposes. Spark runs in the embedded mode with as many worker threads as logical cores on your machine.</p>

<ol>
  <li>Navigate to the <code>&lt;project_root&gt;/word-counter-demo/spark/target</code> directory</li>
  <li>Run the following command <code>java -jar spark-wordcounter.jar -s jini://*/*/space?groups=spark -m local[*]</code></li>
</ol>

<h3 id="option-b-run-spark-standalone-mode-cluster">Option B. Run Spark standalone mode cluster</h3>

<p>In this option Spark runs a cluster in the standalone mode (as an alternative to running on a Mesos or YARN cluster managers).</p>

<h4 id="run-spark">Run Spark</h4>

<ol>
  <li>Download Spark (tested with Spark 1.2.1 pre-built with Hadoop 2.4)</li>
  <li>Follow <a href="http://spark.apache.org/docs/1.2.0/spark-standalone.html">instructions</a> to run a master and 2 workers. Here is an example of commands with hostname <code>fe2s</code> (remember to substitute it with yours)
<code>
./sbin/start-master.sh
./bin/spark-class org.apache.spark.deploy.worker.Worker spark://fe2s:7077
./bin/spark-class org.apache.spark.deploy.worker.Worker spark://fe2s:7077
</code></li>
</ol>

<h4 id="submit-application">Submit application</h4>

<ol>
  <li>Submit an application to Spark (in this example driver runs locally)</li>
  <li>Navigate to the <code>&lt;project_root&gt;/word-counter-demo/spark/target</code> directory</li>
  <li>Run <code>java -jar spark-wordcounter.jar -s jini://*/*/space?groups=spark -m spark://fe2s:7077 -j ./spark-wordcounter.jar</code></li>
  <li>Spark web console should be available at <a href="http://fe2s:8080">http://fe2s:8080</a></li>
</ol>

<h2 id="launch-feeder-application">Launch Feeder application</h2>

<ol>
  <li>Navigate to <code>&lt;project_root&gt;/word-counter-demo/feeder/target</code></li>
  <li>Run <code>java -jar feeder.jar -g spark -n 500</code></li>
</ol>

<p>At this point all components should be up and running. The application is available at <a href="http://localhost:8090/web/">http://localhost:8090/web/</a></p>
]]></content>
  </entry>
  
</feed>
